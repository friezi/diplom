\chapter{Experimente und Auswertung}
\label{c:experimente}
In diesem Kapitel werden wir untersuchen, welche Vorteile das in dieser Arbeit vorgestellte System gegenüber dem herkömmlichen System bietet.
Des Weiteren werden wir das Adaptionsverhalten der beschriebenen Verfahren untersuchen. Dabei werden wir im Detail einzelne Aspekte
der Verfahren genauer betrachten und ihre Auswirkungen im Gesamtkontext darstellen. Wir verwenden die in Kapitel \ref{c:implementierung} vorgestellte
Simulationsumgebung.
Da wir keine Vergleichsmöglichkeiten mit anderen Verfahren haben, werden wir das Verhalten des Systems bei unterschiedlichen Parametern bzw. Algorithmen
untersuchen. Die Ergebnisse der empirischen Untersuchungen dienen dabei der Überprüfung der getroffenen Annahmen, die die Entwicklung unserer
Verfahren motiviert haben.

\section{Aufbau der Experimente}
Damit die Experimente untereinander vergleichbar sind, haben wir eine einheitliche Parameterwahl getroffen. Parameter wurden nur dann gezielt modifiziert,
wenn dies für das jeweilige Experiment von entscheidender Bedeutung war.

\paragraph{Topologien:}
Die Simulationsumgebung besitzt eingebaute Topologien (z. B. $Topology\-One\-Sur\-roun\-ded$, siehe Kapitel \ref{c:implementierung}),
welche sich gut für eine visuelle Kontrolle der Algorithmen eignen,
da sie von ihrer Struktur her einfach und übersichtlich aufgebaut sind. Um jedoch aussagekräftige Ergebnisse zu erhalten, die auch in Hinsicht auf
Netzwerktopologien, so wie sie im Internet vorzufinden sind, als realistisch eingestuft werden können, müssen wir andere Topologien heranziehen. Wir bedienen uns
Topologien, welche auf dem Transit-Stub-Modell \cite{Zegura1996} basieren. Dieses Modell spiegelt sehr gut reale Internetstrukturen wider. Bei diesem Modell
besteht das Netzwerk aus mehreren Domänen, die entweder vom Typ \glqq Stub-Domäne\grqq{} oder \glqq Transit-Domäne\grqq{} sind. Während der Datenverkehr nur dann
durch eine Stub-Domäne fließt, wenn der Ziel- bzw. Ausgangsknoten innerhalb dieser Stub-Domäne liegt, besteht diese Einschränkung für Transit-Domänen nicht.
Transit-Domänen dienen somit dazu, Stub-Domänen miteinander zu verbinden, sie leiten den Datenverkehr weiter. Für die Stub-Domänen bilden sie den Backbone.\\

Es gibt verschiedene Tools, um Topologien basierend auf dem Transit-Stub-Modell zu generieren. Eines davon ist BRITE \cite{Medina:2001BRITE}, welches hier
Verwendung fand, um die notwendigen Topologien zu generieren. Für die Simulation sind zwei Topologien notwendig: eine Sublayer-Topologie, welche
die physischen Verbindungen
zwischen den Knoten darstellt und eine Toplayer-Topologie, welche das Overlay-Netzwerk repräsentiert. Bei allen Experimenten wurde eine Sublayer-Topologie
bestehend aus 2.000 Knoten verwendet. Das Overlay-Netzwerk bestimmt sich dann aus den Broker-Knoten (hier 200 Knoten), welche fest gewählt wurden, und den
Subscriber-Knoten, deren Zahl sich aus der Hälfte der verbleibenden Knoten bestimmt (also 900) und die zufällig den einzelnen Brokern zugewiesen wurden.
Dies geschieht jedoch so,
dass jeder Broker in etwa gleich viele Subscriber verwaltet. Hätten wir keine homogene Verteilung der Subscriber, so würden sich Ballungszentren bilden, in denen
die Verbreitung der RSS-Feeds relativ schnell geschehen kann. Unser Ziel ist es jedoch, das System unter möglichst ungünstigen Bedingungen zu testen.
Aus dem gleichen Grund haben wir uns dazu entschieden, die Lokalität bei der Zuweisung der Subscriber nicht zu berücksichtigen.
Die übrigen Knoten sind lediglich Transfer-Knoten, welche für die Weiterleitung des Datenverkehrs
zuständig sind.\\

Da ein einzelner Durchlauf eines Experimentes keine repräsentativen Ergebnisse liefert, wurde jedes Experiment 30 mal mit unterschiedlichen
Zufallswerten durchgeführt. Dadurch hat sich bei jedem Durchlauf das Overlay-Netzwerk geringfügig verändert (s. o.). Für jeden
gemessenen Wert wurden aus allen Ergebnissen Mittelwerte und Konfidenzintervalle berechnet.

\paragraph{Parameter:}
\begin{table*}
\begin{center}
  \begin{tabular}{|rlr|}
    \hline
    Parameter&Wert&Beschreibung\\
    \hline\hline
    &&\\
    engineTimerPeriod & 5 & interner Parameter\\
    gnuplotTimeStepSecs & 5 & Abtastrate für gemessene Werte in KTicks\\
    &&\\
    maxFeedEvents & 5 & max. Anzahl von Events innerhalb eines Feeds\\
    maxSubscriberEvents & 10 & Anzahl Events, die ein Subscriber vorhält\\
    &&\\
    maxPollingPeriod & 910800 & $mpp$\\
    preferredPollingPeriod & 1 & $ppp$\\ 
    &&\\
    ttl & 5 & $ttl$ pro Feed\\
    &&\\
    serverQueueSize & 40 & Größe der Server-Queue\\
    processingTimeFeedRequest & 350 & Bearbeitungszeit einer Anfrage (Millisek.)\\
    processingTimeUnrepliedRequest & 43 & Verzögerungszeit bei abgewiesenen Anfragen\\
    &&\\
    rssFeedMsgRT & 70 & Basislaufzeit f. RSS-Feeds\\
    rssFeedRequestMsgRT & 50 & Basislaufzeit f. Feed-Requests\\
    &&\\
    \hline
  \end{tabular}
\end{center}
\caption{Standardparameter}
\label{Tab:Standardparameter}
\end{table*}

Zusätzlich zu den oben beschriebenen Parametern für die Netz\-werk-To\-po\-lo\-gien wurden die in Tabelle \ref{Tab:Standardparameter}
angegebenen Parameter als Standardwerte gesetzt.\\

Die tatsächlichen Nachrichtenlaufzeiten berechnen sich aus $engineTimerPeriod\cdot rss\-Feed\-Msg\-RT$\footnote{Der Parameter $engineTimerPeriod$ hat
zwar nur historische Gründe, ist aber zur Berechnung der Nachrichtenlaufzeiten relevant.} bzw. $engineTimerPeriod\cdot rss\-Feed\-Request\-Msg\-RT$.
Wir haben bewusst relativ große
Nachrichtenlaufzeiten gewählt, um das System unter möglichst ungünstigen Bedingungen zu testen. Große Nachrichtenlaufzeiten können bewirken,
dass Antworten eines RSS-Servers zu spät erfolgen und Klienten daher ihre Anfragen wiederholen. Dies führt zunächst zu einer Mehrbelastung
des Servers. Auch wenn in der Realität dauerhaft solch hohe Nachrichtenlaufzeiten nicht auftreten, so lassen sich doch damit die
Adaptionsalgorithmen unter Extrembedingungen testen.\\

Die Zeitdauer bei jeder Messung betrug 6.000 KTicks, wobei gilt: 1 KTick = 1000 Ticks (ein Tick ist der kleinste mögliche Simulationsschritt).

\importsmallgnuplotps{Verbesserung des Aktualitätsgrades $\Gamma_A$}
{Abb:Verbesserung_des_Aktualitaetsgrades_bei_zunehmender_Anzahl_der_Subscriber}{average_up_to_date_percent_improvement}

\section{Experimente -- bevorzugte Polling-Periode}
Alle Experimente wurden in Hinsicht auf bevorzugte Polling-Perioden als angestrebte Dienstgüte durchgeführt. Zunächst werden wir betrachten, welche
Vorteile das vorgestellte Verfahren gegenüber einem Verfahren bietet, bei dem keine Staukontrolle eingesetzt wird. Bei beiden Verfahren profitieren
Klienten von einer Verteilung der Feeds über das Notifikationssystem, doch schon hier lassen sich gravierende Vorteile des vorgestellten Verfahrens
erkennen. Es sollte leicht einzusehen sein, dass diese Vorteile sich noch verstärken, falls kein Notifikationssystem zum Einsatz kommt.\\

Abbildung \ref{Abb:Verbesserung_des_Aktualitaetsgrades_bei_zunehmender_Anzahl_der_Subscriber} zeigt die Verbesserung des Aktualitätsgrades
(Gütefunktion $\Gamma_A$) in Ab\-hän\-gig\-keit von der Anzahl der Klienten im Netzwerk. Dargestellt sind die Konfidenzintervalle mit oberer und unterer Grenze.
Ausgangsparameter für dieses Experiment sind eine Netzgröße von 2001 Knoten,
200 Broker, ppp=1 , rssFeedMsgRT=1 und rssFeedRequestMsgRT=1. Die Nachrichtenlaufzeiten wurden bewusst geringer gesetzt als in den übrigen Experimenten, um bei
diesem Experiment auch für das herkömmliche Verfahren (keine Staukontrolle) zu erreichen, dass keine Verzögerungen der Nachrichten bedingt durch zu hohe
Nachrichtenlaufzeiten auftreten.
Die Zeiten entsprechen eher realen Bedingungen als die Werte der Standardparameter. So kann ein Schwellenwert der Anzahl der Klienten bestimmt werden, bei dem eine
Divergenz der Dienstgüte eintritt. Der Zeitpunkt, zu dem die  Messung der Dienstgüte stattfand, lag bei 6.000 KTicks. In der Abbildung ist zu sehen, dass
bereits bei einer Anzahl von 24 Klienten eine dramatische Verbesserung des Aktualitätsgrades der Nachrichten eintritt. Mit zunehmender Zahl der Klienten im
Netzwerk steigt auch der Grad der Verbesserung der Dienstgüte. Die Ursache dafür ist darin zu sehen, dass mit zunehmender Anzahl der Subscriber die Belastung
des Servers durch vermehrte Anfragen zunimmt. Da bei dem Verfahren ohne Staukontrolle keine Anpassung stattfindet, kann der Server einer zeitgerechten Bearbeitung
nicht mehr nachkommen.
%\importsmallgnuplotps{Verbesserung des Verzögerungsgrades $\Gamma_V$}
%{Abb:Verbesserung_des_Verzoegerungsgrades_bei_zunehmender_Anzahl_der_Subscriber}{average_message_delay_percent_improvement}
\importsmallgnuplotps{Serverbelastung: Referenzverlauf}{Abb:Referenzverlauf}{ToTR_Referenzverlauf_MVR}

\section{Zusätzliche Experimente -- bevorzugte Polling-Periode}
In diesem Abschnitt werden wir einzelne Aspekte der entwickelten Verfahren näher untersuchen. Eine Legende zu den
Diagrammen findet sich im Anhang auf Seite \pageref{legende}.
\subsection{Beispiel und Referenz}
\label{Abschnitt:Referenzgraph}
Zunächst zeigen wir in Abbildung \ref{Abb:Referenzverlauf} ein Beispielergebnis eines Experiments, welches gleichzeitig als Referenz
für einige weitere Experimente dienen soll, da alle in den vorhergehenden Kapiteln beschriebenen Verfahren zur Anwendung kommen. Dargestellt ist
die Entwicklung der Serverbelastung anhand der eingegangenen Feed-Requests. Von den Standardparametern
wurde nicht abgewichen. Die Subscriber treten in einer zufälligen Verteilung in der Zeitspanne $1...1000$ KTicks dem Overlay-Netzwerk bei.
Der Faktor für die Bearbeitungszeit, die ein Server für Anfragen benötigt ($serviceTimeFactor$, siehe Kapitel \ref{c:implementierung}),
ist hierbei 1.\\

Es ist zu erkennen, dass die Kurve bis zum Ende der Beitrittsphase leicht über die obere Grenze der Server-Queue steigt. Nach Abschluss der Beitrittsphase
pegelt sich die Kurve jedoch auf die obere Grenze der Server-Queue ein, was bedeutet dass der RSS-Server an seiner Belastungsgrenze arbeitet
und die mittlere Ankunftsrate der Anfragen in etwa gleich der mittleren Bearbeitungszeit der Anfragen ist.\\
\importsmallgnuplotps{Mittelwerte der Polling-Perioden ($cpp$): Referenz}
{Abb:Referenzverlauf_Mittelwerte_der_Polling-Perioden}{meanValueCPP_Referenzverlauf_MeanValueRanges}
\importsmallgnuplotps{Standardabweichung Mittelwerte $cpp$: Referenz}
{Abb:Standardabweichung_Referenzverlauf_Mittelwerte_der_Polling-Perioden}{stdDevCPP_Referenzverlauf_MeanValueRanges}
\importsmallgnuplotps{Variationskoeffizient Mittelwerte $cpp$: Referenz}
{Abb:Variationskoeffizient_Referenzverlauf_Mittelwerte_der_Polling-Perioden}{coeffVarCPP_Referenzverlauf_MeanValueRanges}
\importsmallgnuplotps{Nachrichtenverzögerung $\Gamma_V$: Referenz}{Abb:Referenzverlauf_Nachrichtenverzoegerung}{avgMsgDelayRatio_Referenzverlauf_MeanValueRanges}
\importsmallgnuplotps{Aktualitätsgrad $\Gamma_A$: Referenz}{Abb:Referenzverlauf_Aktualitaetsgrad}{avgUptodateRatio_Referenzverlauf_MeanValueRanges}
\importsmallgnuplotps{Prozentsatz der gesparten Nachrichten}{Abb:Referenzverlauf_Prozentsatz_der_gesparten_Nachrichten}{relReOmRatio_Referenzverlauf_MeanValueRanges}

Abbildung \ref{Abb:Referenzverlauf_Mittelwerte_der_Polling-Perioden} zeigt die Entwicklung der Mittelwerte der Polling-Perioden aller Subscriber. Nach einem Anstieg
bis ca. zum Zeitpunkt 3000 KTicks zirkulieren die mittleren Polling-Perioden um einen gedachten Mittelwert von ca. 50.000 KTicks.\\

Abbildung \ref{Abb:Standardabweichung_Referenzverlauf_Mittelwerte_der_Polling-Perioden} zeigt die Standardabweichung von den Mittelwerten der Polling-Perioden.
Mit Zunahme der Mittelwerte nimmt also auch in gleichem Maße die Streuung zu. Bemerkenswert ist, dass trotz immer noch starker Schwankungen der Mittelwerte
und einer großen Streuung die Serverbelastung stabil bleibt (siehe Abbildung \ref{Abb:Referenzverlauf}). Um auch die Größe der Mittelwerte zu berücksichtigen,
haben wir in Abbildung \ref{Abb:Variationskoeffizient_Referenzverlauf_Mittelwerte_der_Polling-Perioden} den normierten Wert der
Standardabweichung, den Variationskoeffizienten (Standardabweichung geteilt
durch den Mittelwert\footnote{Entgegen der gängigen Praxis haben wir den Variationskoeffizienten hier nicht in Prozent angegeben. Um den prozentualen
  Variationskoeffizienten zu erhalten, sind die ermittelten Werte mit dem Faktor 100 zu multiplizieren.}), dargestellt. Der Einfluss der Größe eines Mittelwerts 
auf die Streuung wird hierbei unterbunden. Damit sind die Variationskoeffizienten bei unterschiedlichen Messreihen mit verschiedenen Mittelwerten trotzdem
vergleichbar. Es zeigt sich, dass der Variationskoeffizient nach einer gewissen Zeit relativ stabil bleibt und mit der Serverbelastung korreliert.\\

Abbildung \ref{Abb:Referenzverlauf_Nachrichtenverzoegerung} zeigt den Graphen unserer in Abschnitt \ref{angestrebte_dienstguete} definierten Funktion
$\Gamma_V$. Auffällig ist, dass direkt nach Abschluss der Beitrittsphase die Nachrichtenverzögerung am niedrigsten ist (bis auf die
Anfangsphase). Vergleicht man mit Abbildung \ref{Abb:Referenzverlauf}, so ist zu erkennen, dass die Anpassung der Polling-Perioden fast
zeitgleich abgeschlossen ist und der Server zu diesem Zeitpunkt optimal ausgelastet ist.\\

Die Entwicklung korreliert ebenfalls mit dem Graphen unserer in Abschnitt \ref{angestrebte_dienstguete} definierten Funktion
$\Gamma_A$ (siehe Abbildung \ref{Abb:Referenzverlauf_Aktualitaetsgrad}). Die anfängliche Spitze im Graphen der Abbildung
\ref{Abb:Referenzverlauf_Aktualitaetsgrad} ist dadurch zu erklären, dass sich zunächst wenige Subscriber im Netzwerk befinden und der
Server in diesem Zeitbereich noch wenig belastet ist. Daher können die Nachrichten schnell durch das Notifikationssystem
übermittelt werden. Ein Zusammenhang zwischen Serverbelastung, Nachrichtenverzögerung und
Aktualitätsgrad ist also deutlich zu erkennen. \\

Die gezeigten Werte in den Abbildungen \ref{Abb:Referenzverlauf_Nachrichtenverzoegerung} und \ref{Abb:Referenzverlauf_Aktualitaetsgrad} mögen relativ
schlecht erscheinen, jedoch ist zu beachten, dass diese Werte aufgrund der besonderen Parameterwahl zustande kommen. Die starke Verzögerung der Nachrichten
ist beabsichtigt. Relevant ist in diesen Diagrammen lediglich die zeitliche Entwicklung der Werte.\\

Abbildung \ref{Abb:Referenzverlauf_Prozentsatz_der_gesparten_Nachrichten} zeigt den prozentualen Anteil der gesparten Nachrichten im Gegensatz zu einem System ohne
Verteilung der Feeds über ein Notifikationssystem. Grundsätzlich gilt: je geringer die Rate der Anfragen und je größer die Serverbelastung, desto höher ist
der prozentuale Anteil gesparter Nachrichten. Denn in diesen Fällen erhält ein Subscriber einen Feed eher über das Notifikationssystem als direkt vom
RSS-Server. Im Graph in Abbildung \ref{Abb:Referenzverlauf_Prozentsatz_der_gesparten_Nachrichten} ist zu sehen, dass nach Abschluss der Beitrittsphase
zum Zeitpunkt 1000 der Anteil an gesparten Nachrichten wieder leicht sinkt. Diese Abnahme hat ihre Ursache in der geringeren Serverbelastung als im davor
liegenden Zeitraum.
\importsmallgnuplotps{Serverbelastung: Ohne Staukontrolle}{Abb:Ohne_Staukontrolle}{ToTR_NoCongCont_MVR}

\subsection{Staukontrolle}
\importsmallgnuplotps{Aktualitätsgrad $\Gamma_A$ (ohne Staukontrolle)}
{Abb:Ohne_Staukontrolle_Aktualitaetsgrad}{avgUptodateRatio_NoCongCont_MeanValueRanges}
\importsmallgnuplotps{Nachrichtenverzögerung $\Gamma_V$ (ohne Staukontrolle)}
{Abb:Ohne_Staukontrolle_Nachrichtenverzoegerung}{avgMsgDelayRatio_NoCongCont_MeanValueRanges}
Der graphischen Verlauf der Serverbelastung mit Staukontrolle ist dem Referenzgraphen in Abbildung \ref{Abb:Referenzverlauf} zu entnehmen.\\

Man kann sich leicht überlegen, welche Folgen eine fehlende Staukontrolle seitens der Subscriber hat. Die Auswirkungen einer fehlenden Staukontrolle
auf die Serverbelastung haben wir ebenfalls untersucht. In Abbildung \ref{Abb:Ohne_Staukontrolle} ist zu sehen,
dass die Serverbelastung wie erwartet stetig zunimmt.

Der Grad der Nachrichtenverzögerung nimmt ebenfalls stetig zu (siehe Abbildung \ref{Abb:Ohne_Staukontrolle_Nachrichtenverzoegerung}),
wogegen der Aktualitätsgrad praktisch auf null sinkt (siehe Abbildung \ref{Abb:Ohne_Staukontrolle_Aktualitaetsgrad}).

\subsection{Ein Publisher}
\label{Abschnitt:Ein_Publisher}
\suppressfloats
\importsmallgnuplotps{Ein Publisher: Akualitätsgrad $\Gamma_A$}{Abb:Ein_Publisher_Akualitaetsgrad}{avgUptodateRatio_SinglePublisher_MeanValueRanges}
\importsmallgnuplotps{Ein Publisher: Nachrichtenverzögerung $\Gamma_V$}{Abb:Ein_Publisher_Nachrichtenverzoegerung}{avgMsgDelayRatio_SinglePublisher_MeanValueRanges}
Hier untersuchen wir die Adaptionsgüte für den Ansatz, bei dem sich nur ein Publisher im System befindet. Es ist irrelevant,
ob es sich dabei um einen push-basierten Ansatz handelt, bei dem der RSS-Server Feeds selbständig in das Notifikationssystem einspeist, oder ob es sich um einen
pull-Ansatz handelt, bei dem genau ein Klient die Rolle des Publishers übernimmt und in regelmäßigen Zeitabständen den RSS-Server pollt.\\
In diesem Experiment übernimmt ein Klient die Rolle des Publishers und pollt den RSS-Server mit einer Polling-Periode von 1 KTick. Dies entspricht dem naiven
Ansatz, der auch bei CMS-ToPSS  \cite{PetrovicEtAl:2005:CMS-ToPSS} verfolgt wird (siehe Abschnitt \ref{Abschnitt:CMS-ToPSS}). Abbildung
\ref{Abb:Ein_Publisher_Akualitaetsgrad} zeigt die Adaptionsgüte $\Gamma_A$. Der Aktualitätsgrad ist im Schnitt deutlich geringer als bei unserem Referenzexperiment.
Abbildung \ref{Abb:Ein_Publisher_Nachrichtenverzoegerung} zeigt die Nachrichtenverzögerung $\Gamma_V$. Diese ist deutlich höher als bei dem Referenzexperiment.
Auch wenn sich das Experiment auf ein spezielles Szenario bezieht, so macht es deutlich, dass bei ungünstigen Netzwerken (große Netzwerke, geringer Grad der
Verzweigung zwischen Brokern, lange Übertragungszeiten) der in dieser Arbeit vorgestellte Ansatz dem Ansatz mit nur einem Publisher vorzuziehen ist, falls ein hoher
Aktualitätsgrad der RSS-Feeds ausschlaggegebend für die Systemwahl ist. Der Ansatz mit nur einem Publisher hat (je nach Konfiguration) eine geringere
Serverbelastung und eine größere Lastverteilung bezüglich der Datenmengen zur Folge. Hier liegen die Stärken des Ansatzes.


\subsection{Ausbalancierung}

\suppressfloats

\importsmallgnuplotps{Serverbelastung: Keine Ausbalancierung}{Abb:Keine_Ausbalancierung}{ToTR_NoBalancing_MVR}
\importsmallgnuplotps{Serverbelastung: Ausbalancierung}{Abb:Ausbalancierung}{ToTR_Balancing_MVR}
\importsmallgnuplotps{Mittelwerte der Polling-Perioden ($cpp$): keine Ausbalancierung}
{Abb:Keine_Ausbalancierung_Mittelwerte_der_Polling-Perioden}{meanValueCPP_NoBalancing_MeanValueRanges}
\importsmallgnuplotps{Mittelwerte der Polling-Perioden ($cpp$): Ausbalancierung}
{Abb:Ausbalancierung_Mittelwerte_der_Polling-Perioden}{meanValueCPP_Balancing_MeanValueRanges}
\importsmallgnuplotps{Standardabweichung: Mittelwerte $cpp$: k. Ausbalancierung}
{Abb:Keine_Ausbalancierung_Standardabweichung_der_Mittelwerte_der_Polling-Perioden}{stdDevCPP_NoBalancing_MeanValueRanges}
\importsmallgnuplotps{Standardabweichung Mittelwerte $cpp$: Ausbalancierung}
{Abb:Ausbalancierung_Standardabweichung_der_Mittelwerte_der_Polling-Perioden}{stdDevCPP_Balancing_MeanValueRanges}
\importsmallgnuplotps{Variationskoeffizient Mittelwerte $cpp$: k. Ausbalancierung}
{Abb:Keine_Ausbalancierung_Variationskoeffizient_der_Mittelwerte_der_Polling-Perioden}{coeffVarCPP_NoBalancing_MeanValueRanges}
\importsmallgnuplotps{Variationskoeffizient Mittelwerte $cpp$: Ausbalancierung}
{Abb:Ausbalancierung_Variationskoeffizient_der_Mittelwerte_der_Polling-Perioden}{coeffVarCPP_Balancing_MeanValueRanges}

In diesem Experiment soll untersucht werden, wie sich ein Ausbleiben der Ausbalancierung der Polling-Perioden auf die Adaption der Polling-Perioden auswirkt
und ob es zum Effekt des \glqq Aussperrens\grqq{} kommt (siehe Abschnitt \ref{cs:ausbalancierung_der_polling-perioden}). Um den Effekt zu maximieren,
haben wir bei diesem Experiment
auf ein allmähliches Beitreten der Subscriber zum Overlay-Netzwerk verzichtet. Alle Subscriber treten gleichzeitig zu Beginn der Simulation dem Overlay-Netzwerk
bei. Kommt es zum \glqq Aussperren\grqq{}, so sollte dieser Effekt anhand der Anzahl der Anfragen an den RSS-Server deutlich sichtbar sein, denn alle ausgesperrten
Subscriber werden aufgrund ausbleibender Antworten ihre Anfragen wiederholen. Dies muss nahezu gleichzeitig geschehen, da alle Subscriber zur selben Zeit dem
Netzwerk beigetreten sind und ihre $RT$s daher annähernd zur gleichen Zeit ablaufen werden. Die Mittelwerte der Polling-Perioden aller Subscriber sollten sich
kontinuierlich erhöhen. Denn nur eine geringe Zahl der Subscriber hat Zugriff auf den Server und kann ihre Polling-Perioden konstant niedrig halten. Die übrigen
Subscriber werden ihre Polling-Perioden erhöhen.\\
Diese Effekte sind im Graphen in Abbildung \ref{Abb:Keine_Ausbalancierung} gut nachvollziehbar. Die auftretenden Spitzen zeigen, dass zu dieser Zeit sehr viele
Subscriber gleichzeitig Feed-Requests aussenden. Dies kann nur eine Ursache in gleichzeitig ablaufenden $RT$s haben. Die Zeitdifferenzen nehmen exponentiell zu.
Dies entspricht der exponentiellen Zunahme der $rto$s. Nach jeder auftretenden Spitze ist die Queue kaum gefüllt, ihr Füllgrad steigt jedoch daraufhin wieder.
Erklären lässt sich dies folgendermaßen: die nicht ausgesperrten Subscriber müssen ihren $cpp$ ebenfalls drosseln (entsprechend den ausgesperrten Subscribern) und
können diesen daraufhin jedoch allmählich wieder steigern.\\
Schaut man sich die Entwicklung der mittleren Polling-Perioden ($cpp$) in Abbildung
\ref{Abb:Keine_Ausbalancierung_Mittelwerte_der_Polling-Perioden} an, so erkennt man (neben der stetigen Steigerung der Polling-Perioden) eine unmittelbare
Korrelation mit den auftretenden Spitzen im Graphen der Abbildung \ref{Abb:Keine_Ausbalancierung}. Tatsächlich steigern also größtenteils die ausgesperrten
Subscriber ihre Polling-Perioden. Sie erhalten keine Möglichkeit, diese zu senken. Auch die Standardabweichung von den Mittelwerten der Polling-Perioden (Abbildung
\ref{Abb:Keine_Ausbalancierung_Standardabweichung_der_Mittelwerte_der_Polling-Perioden}) unterstützen die These des \glqq Aussperrens\grqq{}. Die Graphik zeigt,
dass die Polling-Perioden der verschiedenen Subscriber stark von einander abweichen. Abbildung
\ref{Abb:Keine_Ausbalancierung_Variationskoeffizient_der_Mittelwerte_der_Polling-Perioden} zeigt den Variationskoeffizienten.\\
Als Vergleichsgraphen haben wir in Abbildung \ref{Abb:Ausbalancierung} den graphischen Verlauf der Serverbelastung mit Ausbalancierung bei ansonsten gleicher
Parameterwahl dargestellt. Bei Vergleich der Graphen der Mittelwerte beider
Verfahren fällt auf, dass sich die Mittelwerte in beiden Fällen (nach völlig unterschiedlichem Verlauf)
etwa bei 100.000 KTicks bewegen. Jedoch haben sich die Mittelwerte bei dem ausbalancierten Verfahren nach unten hin eingepegelt (Abbildung
\ref{Abb:Ausbalancierung_Mittelwerte_der_Polling-Perioden}), während die Mittelwerte beim nicht-ausbalancierten Verfahren weiterhin stetig steigen
(Abbildung \ref{Abb:Keine_Ausbalancierung_Mittelwerte_der_Polling-Perioden}).
Ein großer Unterschied zwischen beiden Verfahren ist bei dem Vergleich der Standardabweichungen zu erkennen: beim nicht-ausbalancierten Verfahren
(Abbildung \ref{Abb:Keine_Ausbalancierung_Standardabweichung_der_Mittelwerte_der_Polling-Perioden}) hat zum Ende der Simulation die Standardabweichung
den Wert 180.000 erreicht und ist weiter steigend; beim ausbalancierten Verfahren (Abbildung
\ref{Abb:Ausbalancierung_Standardabweichung_der_Mittelwerte_der_Polling-Perioden}) hat sich die Standardabweichung nach unten hin
auf ca. den Wert 100.000 eingepegelt. Der Variationskoeffizient des ausbalancierten Verfahrens
(Abbildung \ref{Abb:Ausbalancierung_Variationskoeffizient_der_Mittelwerte_der_Polling-Perioden}) pegelt sich ebenfalls auf einem niedrigeren Wert ein,
als der Variationskoeffizient des nicht-ausbalancierten Verfahrens
(Abbildung \ref{Abb:Keine_Ausbalancierung_Variationskoeffizient_der_Mittelwerte_der_Polling-Perioden}) .

\subsection{Churn-Kompensation}
\label{exp:churn_kompensation}

\suppressfloats

\importsmallgnuplotps{Serverbelastung: Ohne Churn-Kompensation}{Abb:ohne_Churn-Kompensation}{ToTR_NoChurnCompensation50_80_900_MVR}
\importsmallgnuplotps{Serverbelastung: Mit Churn-Kompensation}{Abb:mit_Churn-Kompensation}{ToTR_ChurnCompensation50_80_900_MVR}
\importsmallgnuplotps{Churn-Kompensation: Mittelwerte abgewiesener Anfragen}{Abb:Churn-Kompensation_Mittelwerte_abgewiesener_Anfragen}
{churn_meanValues}
\importsmallgnuplotps{Churn-Kompensation: Spitzenwerte abgewiesener Anfragen}{Abb:Churn-Kompensation_Spitzenwerte_abgewiesener_Anfragen}
{churn_peeks}

\importsmallgnuplotps{Serverbelastung: Exponentielle Skalierungsfunktion}
{Abb:Exponentielle_Skalierungsfunktion}{ToTR_ExponentialArttFactor_serviceTimeFactor400_MVR}
\importsmallgnuplotps{Serverbelastung: Polynomielle Skalierungsfunktion}
{Abb:Polynomielle_Skalierungsfunktion}{ToTR_QuadraticArttFactor_serviceTimeFactor400_MVR}

Mit Hilfe dieses Experiments soll der Einflusses von Churn auf die Serverbelastung sowohl mit als auch ohne Churn-Kompensation ermittelt werden.
Je stärker die schon bestehende Serverbelastung bzw. je weniger leistungsfähig ein Server ist, desto größer ist der Einfluss von Churn.
Damit der Einfluss von Churn auf die Serverbelastung deutlich hervor tritt, wurde in diesem Experiment ein $serviceTimeFactor$ von 50 gewählt.
Die Nachrichtenlaufzeit ist von entscheidender Bedeutung für die Churn-Kompensation: ist sie zu hoch, dann benötigt die Übermittlung des
$InitialBrokerRSSFeed$ sehr viel Zeit, so dass ein Subscriber zwischenzeitlich bereits den RSS-Server kontaktiert.
Die gewählten Werte für Nachrichtenlaufzeiten sind im Vergleich zu in der Realität auftretenden Werten sehr hoch gewählt.
Allerdings wollen wir das System unter möglichst ungünstigen Bedingungen testen. Bei geringeren Übertragungsgeschwindigkeiten verbessert sich
die Auswirkung der Churn-Kompensation. Die hier dargestellten Werte sind Extremwerte und sollten unter realen Bedingungen weit besser ausfallen.\\

Churn beginnt beim Zeitpunkt 3000 und endet beim Zeitpunkt 5000. Dabei sind nach 900 KTicks 80\% der Klienten ausgetauscht.
Abbildung \ref{Abb:ohne_Churn-Kompensation} zeigt den Einfluss von Churn auf die Serverbelastung ohne Churn-Kompensation. Zu erkennen ist eine starke
und stetig ansteigende Serverbelastung. Nur ab etwa dem Zeitpunkt 4600 KTicks fällt die Kurve wieder leicht ab. Nach Beendigung der Churnphase
pegelt sich die Kurve langsam wieder in Richtung Server-Queue ein. Dagegen zeigt der Graph mit Churn-Kompensation in
Abbildung \ref{Abb:mit_Churn-Kompensation} eine weit geringere Serverbelastung und ein besseres Adaptionsverhalten, obwohl auch hier der Einfluss
von Churn nicht ganz vermeidbar ist. Es fällt weiterhin auf, dass durch die Churn-Kompensation schon in der Beitrittsphase (0...1000 KTicks)
ein besseres Adaptionsverhalten erreicht wird.\\

Das beschriebene Experiment stellt nur ein ausgewähltes Beispiel für die Entwicklung der Serverbelastung mit und ohne Churn-Kompensation dar.
Um zu ermitteln, wie der Grad der Churn-Kompensation mit der Churnrate zusammenhängt, wurden weitere Experimente durchgeführt, deren Ergebnisse
in den Abbildungen \ref{Abb:Churn-Kompensation_Mittelwerte_abgewiesener_Anfragen} und \ref{Abb:Churn-Kompensation_Spitzenwerte_abgewiesener_Anfragen}
veranschaulicht sind. Dabei wird die Serverbelastung bzw. die Anzahl abgewiesener Anfragen in Abhängigkeit der prozentualen Angabe der Subscriber
dargestellt, die innerhalb einer festen Zeit (900 KTicks) das System dynamisch betreten und verlassen. Abbildung
\ref{Abb:Churn-Kompensation_Mittelwerte_abgewiesener_Anfragen} zeigt die Mittelwerte abgewiesener Anfragen innerhalb der definierten Churnphase.
Abbildung \ref{Abb:Churn-Kompensation_Spitzenwerte_abgewiesener_Anfragen} zeigt die Spitzenwerte. In Abbildung
\ref{Abb:Churn-Kompensation_Mittelwerte_abgewiesener_Anfragen} ist ein annähernd linearer Kurvenverlauf zu sehen. In
\ref{Abb:Churn-Kompensation_Spitzenwerte_abgewiesener_Anfragen} ist zu erkennen, dass die Kurve ab etwa 70\% nach oben hin ansteigt.
Das lässt sich dadurch erklären, dass die Wahrscheinlichkeit zunimmt, dass Anfragen verschiedener Klienten zeitlich
zusammenfallen, je größer die Churnrate ist.\\

Obwohl sich in der Simulation Churn auch trotz Churn-Kompensation auf die Serverbelastung nachteilig auswirkt, sollte dieses Phänomen in der Praxis
nur unter der Bedingung auftreten, dass die Broker selbst stark überlastet sind und nicht in angemessener Zeit antworten können, so dass ein
Timeout auf TCP-Ebene eintritt. Kommt ein Datentransfer über
eine TCP-Verbindung zwischen Klient und Broker zustande, so kann die negative Auswirkung von Churn nicht mehr eintreten, auch wenn die Dauer der Datenübertragung
mehr Zeit beansprucht, als die Länge der Polling-Periode des jeweiligen Klienten. Denn erst nach Abschluss der Datenübertragung wird der Timer
$RQT$ des Klienten gesetzt. Eine bestehende (wenn auch langsame) Datenübertragung wird seitens des Klienten nicht unterbrochen.\\

Auf die Entwicklung der Serverbelastung bei Wahl der Standard-Parameter hat Churn keinen nennenswerten Einfluss (hier nicht dargestellt).

\subsection{Bestimmung des $artt$ -- Skalierungsfunktion für den $rtt$}
\label{exp:skalierung}

Bei diesem Experiment soll die Adaptionsgüte der Polling-Perioden anhand der Serverbelastung bei unterschiedlicher Wahl der Skalierungsfunktion
sf() (siehe Abschnitt \ref{css:staukontrolle_pubsubrss}) ermittelt werden. Wir beschränken uns dabei auf den Vergleich exponentieller und
polynomieller Funktionen. Auch hier haben wir einen $serviceTimeFactor$ von 400 gewählt, um Unterschiede in der Entwicklung deutlicher sichtbar
zu machen. Abbildung \ref{Abb:Exponentielle_Skalierungsfunktion} zeigt den Verlauf der Serverbelastung bei Wahl der
Skalierungsfunktion $\text{sf}(x):=2^{x-1}$. In Abbildung \ref{Abb:Polynomielle_Skalierungsfunktion} ist der Verlauf der Serverbelastung bei
$\text{sf}(x):=x^2$ zu sehen. Unterschiede im Verlauf sind zwar sichtbar, der Vorteil der exponentiellen Skalierungsfunktion ist jedoch
relativ klein.

\subsection{Reaktion auf Netzwerkveränderung}
\importsmallgnuplotps{Plötzliche Veränderung der Netzwerkgröße}{Abb:Ploetzliche_Veraenderung_der_Netzwerkgroesse}{ToTR_SubscribersLeave50_MVR}

Dieses Experiment dient zur Untersuchung, wie das System auf plötzliche Veränderungen der Netzwerkgröße (Anzahl der Klienten im System)
reagiert. Es ist wünschenswert, dass sich das System schnell an die veränderte Situation anpasst und es dabei nicht zu Seiteneffekten
(wie z. B. zu einem starken Schwingungsverhalten) kommt. In einem Beispielexperiment verlassen in der Simulation die Hälfte der Klienten zum Zeitpunkt
2000 das Overlay-Netzwerk. Zum Zeitpunkt 4000 tritt die gleiche Anzahl an Klienten dem Netzwerk wieder bei. Abbildung
\ref{Abb:Ploetzliche_Veraenderung_der_Netzwerkgroesse} zeigt den graphischen Verlauf. Es ist zu erkennen, dass das Verlassen der Klienten
nur eine kurzzeitige Senkung des Füllgrades der Server-Queue bewirkt. Der Pegel steigt anschließend wieder bis zur oberen Grenze.
Der Beitritt der Klienten zum System bewirkt zwar eine kurzzeitige starke Mehrbelastung des Servers, das System kann diese Mehrbelastung jedoch
relativ schnell ausgleichen.\\

Um allgemeingültigere Aussagen treffen zu können, haben wir die Entwicklung von Serverbelastung und Adaptionszeiten in Abhängigkeit von der prozentualen Anzahl
der Subscriber untersucht, die das System verlassen haben bzw. vorübergehend nicht erreichbar waren.
In Abbildung \ref{Abb:Netzwerkgroesse_Spitzenwerte_abgewiesene_Anfragen} sind die Spitzenwerte abgewiesener Nachrichten nach einem
Wiedereintritt der Subscriber dargestellt.
Abbildung \ref{Abb:Netzwerkgroesse_Adaptionszeiten_nach_Wiedereintritt} zeigt die Zeitdauern der Adaptionen der Polling-Perioden nach einem Wiedereintritt.
In beiden Graphiken ist ein annähernd linearer Verlauf der Kurven zu erkennen.

\suppressfloats

\importsmallgnuplotps{Netzwerkgröße: Spitzenwerte abgewiesener Anfragen}{Abb:Netzwerkgroesse_Spitzenwerte_abgewiesene_Anfragen}{subscribersRejoin_peeks}
\importsmallgnuplotps{Netzwerkgröße: Adaptionszeiten nach Wiedereintritt}{Abb:Netzwerkgroesse_Adaptionszeiten_nach_Wiedereintritt}{subscribersRejoin_adaptionTime}

\importsmallgnuplotps{Serverbelastung: Exponentielle Steigerung des $rto$}{Abb:Exponentielle_Steigerung_des_rto}{ToTR_Exponential_MVR}
\importsmallgnuplotps{Serverbelastung: Polynomielle Steigerung des $rto$}{Abb:Polynomielle_Steigerung_des_rto}{ToTR_Polynomial_MVR}

\importsmallgnuplotps{Serverbelastung: Queuegröße 3000}{Abb:Queuegroesse_3000}{ToTR_q3000_MVR}
\importsmallgnuplotps{Mittelwerte $cpp$: Queuegröße 3000}{Abb:Mittelwerte_Queuegroesse_3000}{meanValueCPP_queue3000_MeanValueRanges}
\importsmallgnuplotps{Standardabweichung Mittelwerte $cpp$: Queuegröße 3000}{Abb:Standardabweichung_Mittelwerte_Queuegroesse_3000}
{stdDevCPP_queue3000_MeanValueRanges}
\importsmallgnuplotps{Variationskoeffizient Mittelwerte $cpp$: Queuegröße 3000}{Abb:Variationskoeffizient_Mittelwerte_Queuegroesse_3000}
{coeffVarCPP_queue3000_MeanValueRanges}

\subsection{Steigerung des $rto$}
Hier soll gezeigt werden, dass eine exponentielle Steigerung des $rto$ ebenfalls ein besseres Adaptionsverhalten als eine polynomielle Steigerung
nach sich zieht.
Für dieses Experiment wurde zunächst ein $serviceTimeFactor$ von 1 gewählt. Nach 2000 KTicks wird dieser auf 400 hochgesetzt, so dass der Server
nur noch stark zeitverzögert antworten kann. Abbildung \ref{Abb:Exponentielle_Steigerung_des_rto} zeigt die Adaptionsgüte bei exponentieller Steigerung,
Abbildung \ref{Abb:Polynomielle_Steigerung_des_rto} die Adaptionsgüte bei polynomieller Steigerung des $rto$. Hier ist ein deutlich besserer
Verlauf der Adaption bei exponentieller Steigerung zu sehen.

\subsection{Queuegrößen}
Bisher haben wir eine im Verhältnis zu Anzahl der Klienten sehr kleine Queuegröße für die Server-Queue gewählt. In diesem Experiment wollen wir
untersuchen, wie sich eine große Queue auf das Adaptionsverhalten auswirkt. Dafür haben wir eine Queue gewählt, welche 3000 Feed-Requests
aufnehmen kann, also mehr, als sich Klienten im Netzwerk befinden. Ist die Queue groß genug, so sollten nur wenige oder gar keine Feed-Requests
verworfen werden, da die Klienten genug Spielraum haben, um ihre Polling-Perioden anzupassen. Es ist zu erwarten, dass die Polling-Perioden
im weiteren Verlauf nur geringen Schwankungen unterliegen, da nur verworfene Feed-Requests zu zufälligen und unverhältnismäßig großen
Steigerungen führen. Weiterhin sollten die Differenzen der Polling-Perioden zwischen den verschiedenen Klienten relativ gering sein,
da die Bedingungen für alle Subscriber gleich sind und es nicht zu zufälligen Steigerungen kommen kann. Abbildung \ref{Abb:Queuegroesse_3000}
zeigt die Entwicklung der Adaption. Es treten keine verworfenen Feed-Requests auf. Nach einer gewissen Zeit (etwa 5000 KTicks) hat sich die
Kurve auf einem festen Wert (1500) eingepegelt. Das bedeutet, dass ab diesem Zeitpunkt ein ausgewogenes Verhältnis zwischen
mittlerer Ankunftsrate der Feed-Requests und mittlerer Bearbeitungszeit erreicht ist. Abbildung \ref{Abb:Mittelwerte_Queuegroesse_3000}
zeigt die Entwicklung der Mittelwerte der Polling-Perioden. Hier ist zu sehen, dass sich die Mittelwerte entsprechend unseren Erwartungen
auf einem Wert (etwa 2000) einpegeln und keinen weiteren Schwankungen unterliegen. Die Abbildungen
\ref{Abb:Standardabweichung_Mittelwerte_Queuegroesse_3000} und \ref{Abb:Variationskoeffizient_Mittelwerte_Queuegroesse_3000}
machen deutlich, dass die Abweichungen von den Mittelwerten relativ gering sind. Besonders im Vergleich mit den Referenzgraphen (Abbildungen
\ref{Abb:Referenzverlauf_Mittelwerte_der_Polling-Perioden}, \ref{Abb:Standardabweichung_Referenzverlauf_Mittelwerte_der_Polling-Perioden}
und \ref{Abb:Variationskoeffizient_Referenzverlauf_Mittelwerte_der_Polling-Perioden}) zeigt sich, dass bei der Wahl einer großen Queue
(bei ansonsten gleicher Parameterwahl) die Mittelwerte und die Streuungen viel geringere Werte annehmen und es somit zu einer
homogeneren Verteilung der Mittelwerte auf die verschiedenen Klienten kommt.

\subsection{Reaktion auf Belastungsänderung}
Hier soll untersucht werden, wie das System auf Änderungen des Belastungsgrades eines Servers reagiert. Dabei bestehen je nach Dauer der
Belastungsänderung unterschiedliche Anforderungen an das System.

\importsmallgnuplotps{Serverbelastung: langzeitige Überbelastung}{Abb:langzeitige_Ueberbelastung}{ToTR_FastIncreasing_MVR}
\importsmallgnuplotps{Serverbelastung: langzeitige Belastungsminderung}{Abb:langzeitige_Belastungsminderung}{ToTR_FastDegrading_MVR}
\importsmallgnuplotps{Serverbelastung: kurzzeitige Überbelastung}{Abb:kurzzeitige_Ueberbelastung}{ToTR_SuddenIncreaseDecrease40_MVR}
\importsmallgnuplotps{Serverbelastung: kurzzeitige Belastungsminderung}{Abb:kurzzeitige_Belastungsminderung}{ToTR_SuddenDecreaseIncrease40_MVR}
\importsmallgnuplotps{Serverbelastung: allmähliche Überbelastung}{Abb:allmaehliche_Ueberbelastung}{ToTR_SoftIncreasing_MVR}
\importsmallgnuplotps{Serverbelastung: allmähliche Minderbelastung}{Abb:allmaehliche_Minderbelastung}{ToTR_SoftDegrading_MVR}
\paragraph{Langzeitige Belastungsänderungen:}
Bei einer langzeitigen Belastungsänderung eines Servers fordern wir vom System (bzw. von den Klienten) eine möglichst rasche Adaption an die
neuen Bedingungen. Abbildung \ref{Abb:langzeitige_Ueberbelastung} zeigt den Verlauf der Adaption bei einer 50-fachen Steigerung der
Serverbelastung ($serviceTimeFactor=50$).
Zu erkennen ist ein logarithmischer Verlauf der Kurve, welcher unseren Erwartungen entspricht, da sowohl die Steigerung des $rto$ als auch
die Skalierungsfunktion $\text{sf}()$ exponentiell gewählt sind (Abbildung \ref{Abb:Exponentielle_Steigerung_des_rto} des vorigen Abschnitts
zeigt den Verlauf bei einer 400-fach gesteigerten Serverbelastung). Den umgekehrten Fall (Wechsel von 50-facher auf Standard-Serverbelastung)
spiegelt Abbildung \ref{Abb:langzeitige_Belastungsminderung} wider. Die Anpassung geschieht deutlich schneller, da Klienten ihren $cpp$
nicht allmählich drosseln, sondern diesen auf die tatsächlich gemessene roundtrip-time setzen.

\paragraph{Kurzzeitige Belastungsänderungen:}
Ist ein Server für einen
kurzen Zeitraum nicht in der Lage, Anfragen in angemessener Zeit zu bearbeiten, so wäre es nicht wünschenswert, wenn das System (bzw. die Klienten)
mit einer starken und länger andauernden Reaktion überreagiert. Abbildung \ref{Abb:kurzzeitige_Ueberbelastung} zeigt die Reaktion des Systems
auf eine 50-fach gesteigerte Serverbelastung, welche nach 40 KTicks auf den ursprünglichen Wert zurückgesetzt wurde.
Wie zu sehen ist, ändert sich der Verlauf der Kurve nur ganz geringfügig. Abbildung
\ref{Abb:kurzzeitige_Belastungsminderung} zeigt auch hier den umgekehrten Fall bei einem ursprünglichen $serviceTimeFactor$ von 50.
Kurzzeitige Belastungsminderungen sind allerdings in der Praxis eher weniger wahrscheinlich.

\suppressfloats

\importsmallgnuplotps{Serverbelastung: $mpp=3600$}{Abb:mpp_3600}{ToTR_mpp3600_serviceTimeFactor50_MVR}
\importsmallgnuplotps{Serverbelastung: $mpp=910800$}{Abb:mpp_910800}{ToTR_mpp910800_serviceTimeFactor50_MVR}

\paragraph{Allmähliche Belastungsänderungen:}
Bei einer allmählichen Belastungsänderung fordern wir, dass sich das System so schnell an den veränderten Belastungsgrad anpasst, dass
eine zusätzliche Mehr- bzw. Minderbelastung ausbleibt. Abbildungen \ref{Abb:allmaehliche_Ueberbelastung} und
\ref{Abb:allmaehliche_Minderbelastung} zeigen als Beispiele die Graphen der Adaptionsverläufe bei allmählicher Mehr- bzw. Minderbelastung
bei einer graduellen Steigerung des $serviceTimeFactors$ um den Wert 1 alle 40 KTicks. Je kleiner die graduelle Steigerung
bzw. je größer das Zeitintervall zwischen den graduellen Änderungen, desto homogener und unauffälliger ist die Adaption. 

\subsection{Größe des $mpp$}
Damit die Polling-Perioden nicht in unermessliche Höhen schießen (beispielsweise aufgrund von Fehlfunktionen), haben wir den $mpp$
definiert. Dieser sollte groß genug gewählt werden, so dass eine Adaption unter jeden Umständen vollständig ist. Wie sich ein zu
gering gewählter $mpp$ auf das Adaptionsverhalten auswirkt, sehen wir exemplarisch in Abbildung \ref{Abb:mpp_3600} bei einem
$mpp$ von 3600 und einem $serviceTimeFactor$ von 50. Den Verlauf der Adaption bei $mpp=910800$ unter ansonsten gleichen Bedingungen ist in Abbildung
\ref{Abb:mpp_910800} zu sehen. Bei $mpp=3600$ wird eine vollständige Adaption verhindert, da der $cpp$ eines Klienten nicht groß genug
gewählt werden kann.

\subsection{Kurze Nachrichtenlaufzeiten}
\importsmallgnuplotps{$\Gamma_A$: Nachrichtenlaufzeit = 1 KTick}{Abb:Gamma_A_NLZ_1_sekunde}
{avgUptodateRatio_Referenzverlauf_MessageRuntime_1second_MeanValueRanges}
Bisher haben wir fast alle Experimente mit relativ hohen Nachrichtenlaufzeiten durchgeführt, um z. B. eventuelle Bearbeitungszeiten bei Brokern oder
eine verminderte Leistungsfähigkeit bei anderen auf dem Weg zwischen Sender und Empfänger liegenden Knoten zu simulieren. In diesem Experiment wollen
wir darstellen, wie sich kurze Nachrichtenlaufzeiten auf die Anpassung der Polling-Perioden und auf die Adaptionsgüte\footnote{Wir beschränken uns dabei auf
die Adaptionsgüte $\Gamma_A$.} im Vergleich zum Referenzgraphen (siehe Abschnitt \ref{Abschnitt:Referenzgraph}) auswirken. Im Vergleich zum Referenzgraphen
(siehe Abbildung \ref{Abb:Referenzverlauf_small}) ist zu erkennen, dass zu Beginn der Simulation geringfügig mehr Zeit bis zum Erreichen des maximalen
Füllgrades der Serverqueue benötigt wird (siehe Abbildung \ref{Abb:Serverbelastung_NLZ_1_sekunde}). Dies bedeutet, dass der Server in dieser Zeitspanne einer
geringeren Belastung unterliegt. Die Mehrbelastung im
Referenzgraphen ist dadurch zu erklären, dass Subscriber wiederholte Anfragen stellen, bevor eine Antwort des Servers (aufgrund der längeren Nachrichtenlaufzeiten)
den Subscriber erreicht. Auch innerhalb der Joinphase unterliegt der Server einer höheren Belastung im Referenzgraphen. Durch die langen Nachrichtenlaufzeiten hat
die Churn-Kompensation einen geringeren Einfluss auf die Anpassung der Pollingperioden als im Graphen \ref{Abb:Serverbelastung_NLZ_1_sekunde}, da Antworten der
entsprechenden Broker einen Subscriber bisweilen zu spät erreichen. Im weiteren Verlauf der Graphen ist zu erkennen, dass der Graph in Abbildung

\importsmallgnuplotps{Serverbelastung: Nachrichtenlaufzeit = 1 KTick}{Abb:Serverbelastung_NLZ_1_sekunde}
{ToTR_Referenzverlauf_MessageRuntime_1second_MVR}
\importsmallgnuplotps{Serverbelastung: Referenzverlauf}{Abb:Referenzverlauf_small}
{ToTR_Referenzverlauf_MVR_small}

\ref{Abb:Serverbelastung_NLZ_1_sekunde} dichter an der Queuegröße liegt, als der Graph des Referenzverlaufs. Bei kurzen Nachrichtenlaufzeiten unterliegt der Server
hier also einer geringeren Belastung, als bei längeren Nachrichtenlaufzeiten. Abbildung \ref{Abb:Gamma_A_NLZ_1_sekunde} zeigt bei kurzer Nachrichtenlaufzeit den
Verlauf der Adaptionsgüte $\Gamma_A$, welche sich schnell bei 100\% einpegelt.\\
Um diese Arbeit in angemessenem Rahmen zu halten, haben wir auf weitere Untersuchungen und Analysen bezüglich Nachrichtenlaufzeiten verzichtet. Interessant wäre
jedoch zu untersuchen, wie der Grad der Adaption (also die resultierende Serverbelastung) von der Größe der Nachrichtenlaufzeiten abhängt.

%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "diplomarbeit"
%%% End: 
