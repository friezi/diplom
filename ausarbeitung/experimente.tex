\chapter{Experimente und Auswertung}
\label{c:experimente}
In diesem Kapitel werden wir das Adaptionsverhalten der beschriebenen Verfahren untersuchen. Dabei werden wir im Detail einzelne Aspekte der Verfahren genauer
betrachten und ihre Auswirkungen im Gesamtkontext darstellen. Wir bedienen uns dabei der in Kapitel \ref{c:implementierung} vorgestellten Simulationsumgebung. Da
wir keine Vergleichsmöglichkeiten mit anderen Verfahren haben, werden wir das Verhalten des Systems bei unterschiedlichen Parametern bzw. Algorithmen
untersuchen. Die Ergebnisse der empirischen Untersuchungen dienen dabei der Überprüfung der getroffenen Annahmen, die die Entwicklung unserer
Verfahren motiviert haben.

\section{Aufbau der Experimente:}
Damit die Experimente untereinander vergleichbar sind, haben wir eine einheitliche Parameterwahl getroffen. Parameter wurden nur dann gezielt modifiziert,
wenn dies für das jeweilige Experiment von entscheidender Bedeutung war.

\paragraph{Topologien:}
Die Simulationsumgebung besitzt eingebaute Topologien (z. B. $Topology\-One\-Sur\-roun\-ded$, siehe Kapitel \ref{c:implementierung}),
welche gut für eine visuelle Kontrolle der Algorithmen geeignet
sind, da sie von ihrer Struktur her einfach und übersichtlich aufgebaut sind. Um jedoch aussagekräftige Ergebnisse zu erhalten, die auch in Hinsicht auf
Netzwerktopologien, so wie sie im Internet vorzufinden sind, als realistisch eingestuft werden können, müssen wir andere Topologien heranziehen. Wir bedienen uns
Topologien, welche auf dem Transit-Stub-Modell \cite{Zegura1996} basieren. Dieses Modell spiegelt sehr gut reale Internetstrukturen wider. Bei diesem Modell
besteht das Netzwerk aus mehreren Domänen, die entweder vom Typ ``Stub-Domäne'' oder ``Transit-Domäne'' sind. Während der Datenverkehr nur dann durch eine
Stub-Domäne fließt, wenn der Ziel- bzw. Ausgangsknoten innerhalb dieser Stub-Domäne liegt, besteht diese Einschränkung für Transit-Domänen nicht. Transit-Domänen
dienen somit dazu, Stub-Domänen miteinander zu verbinden, sie leiten den Datenverkehr weiter. Für die Stub-Domänen bilden sie Backbones.\\

Es gibt verschiedene Tools, um Topologien basierend auf dem Transit-Stub-Modell zu generieren. Eines davon ist BRITE \cite{Medina:2001BRITE}, welches hier
Verwendung fand, um die notwendigen Topologien zu generieren. Für die Simulation sind zwei Topologien notwendig: eine Sublayer-Topologie, welche
die physischen Verbindungen
zwischen den Knoten darstellt und eine Toplayer-Topologie, welche das Overlay-Netzwerk repräsentiert. Bei allen Experimenten wurde eine Sublayer-Topologie
bestehend aus 2000 Knoten verwendet. Das Overlay-Netzwerk bestimmt sich dann aus den Broker-Knoten (hier 200 Knoten), welche fest gewählt wurden, und den
Subscriber-Knoten, deren Zahl sich aus der Hälfte der verbleibenden Knoten bestimmt (also 900) und die zufällig den einzelnen Brokern zugewiesen wurden.
Dies geschieht jedoch so,
dass jeder Broker in etwa gleich viele Subscriber verwaltet. Die übrigen Knoten sind lediglich Transfer-Knoten, welche für die Weiterleitung des Datenverkehrs
zuständig sind.\\

Da ein einzelner Durchlauf eines Experimentes keine repräsentativen Ergebnisse liefert, wurde jedes Experiment 30 mal mit unterschiedlichen
Zufallswerten durchgeführt. Dadurch hat sich bei jedem Durchlauf das Overlay-Netzwerk geringfügig verändert (s. o.). Für jeden
gemessenen Wert wurden aus allen Ergebnissen Mittelwerte und Konfidenzintervalle berechnet.

\paragraph{Parameter:}
Zusätzlich zu den oben beschriebenen Parametern für die Netz\-werk-To\-po\-lo\-gien wurden die in Tabelle \ref{Tab:Standardparameter}
angegebenen Parameter als Standardwerte gesetzt.

\begin{table*}
\begin{center}
  \begin{tabular}{|rlr|}
    \hline
    Parameter&Wert&Beschreibung\\
    \hline\hline
    &&\\
    engineTimerPeriod & 5 & Laufzeitfaktor für Nachrichtengeschwindigkeiten\\
    gnuplotTimeStepSecs & 5 & Abtastrate für gemessene Werte in Sekunden\\
    &&\\
    maxFeedEvents & 5 & max. Anzahl von Events innerhalb eines Feeds\\
    maxSubscriberEvents & 10 & Anzahl Events, die ein Subscriber vorhält\\
    &&\\
    maxPollingPeriod & 910800 & $mpp$\\
    preferredPollingPeriod & 1 & $ppp$\\ 
    &&\\
    ttl & 5 & $ttl$ pro Feed\\
    &&\\
    serverQueueSize & 40 & Größe der Server-Queue\\
    processingTimeFeedRequest & 350 & Bearbeitungszeit einer Anfrage (Millisek.)\\
    processingTimeUnrepliedRequest & 43 & Verzögerungszeit bei abgewiesenen Anfragen\\
    &&\\
    rssFeedMsgRT & 70 & Basislaufzeit f. RSS-Feeds\\
    rssFeedRequestMsgRT & 50 & Basislaufzeit f. Feed-Requests\\
    &&\\
    \hline
  \end{tabular}
\end{center}
\caption{Standardparameter}
\label{Tab:Standardparameter}
\end{table*}

Die tatsächlichen Nachrichtenlaufzeiten berechnen sich aus $engineTimerPeriod\cdot Basis\-lauf\-zeit$ (bezogen auf den Nachrichtentyp,
siehe Tabelle \ref{Tab:Standardparameter}). Wir haben bewusst relativ hohe
Nachrichtenlaufzeiten gewählt, um das System unter sehr ungünstigen Bedingungen zu testen. Hohe Nachrichtenlaufzeiten können bewirken,
dass Antworten eines RSS-Servers zu spät erfolgen und Klienten daher ihre Anfragen wiederholen. Dies führt zunächst zu einer Mehrbelastung
des Servers. Auch wenn in der Realität dauerhaft solch hohe Nachrichtenlaufzeiten nicht auftreten, so lassen sich doch damit die
Adaptionsalgorithmen unter Extrembedingungen testen.\\

Die Zeitdauer bei jeder Messung betrug 6000 Sekunden.

\section{Experimente -- bevorzugte Polling-Periode}
Alle Experimente wurden in Hinsicht auf bevorzugte Polling-Perioden als angestrebte Dienstgüte durchgeführt. Eine Legende zu den
Diagrammen findet sich im Anhang auf Seite \pageref{legende}.

\subsection{Beispiel und Referenz}
\importsmallgnuplotps{Serverbelastung: Referenzverlauf}{Abb:Referenzverlauf}{ToTR_Referenzverlauf_MVR}

Zunächst zeigen wir in Abbildung \ref{Abb:Referenzverlauf} ein Beispielergebnis eines Experiments, welches gleichzeitig als Referenz
für einige Experimente dienen soll, da alle in den vorherigen Kapiteln beschriebenen Verfahren zur Anwendung kamen. Dargestellt ist
die Entwicklung der Serverbelastung anhand der eingegangenen Feed-Requests. Von den Standardparametern
wurde nicht abgewichen. Die Subscriber treten in einer zufälligen Verteilung in der Zeitspanne $1..1000$ Sekunden dem Overlay-Netzwerk bei.
Der Faktor für die Bearbeitungszeit, die ein Server für Anfragen benötigt ($serviceTimeFactor$, siehe Kapitel \ref{c:implementierung}),
ist hierbei 1.\\

Es ist zu erkennen, dass die Kurve bis zum Ende der Beitrittsphase leicht über die obere Grenze der Server-Queue steigt. Nach Abschluss der Beitrittsphase
pegelt sich die Kurve jedoch auf die obere Grenze der Server-Queue ein, was bedeutet dass der RSS-Server an seiner Belastungsgrenze arbeitet
und die mittlere Ankunftsrate der Anfragen in etwa gleich der mittleren Bearbeitungszeit der Anfragen ist.\\
\importsmallgnuplotps{Mittelwerte der Polling-Perioden ($cpp$): Referenz}
{Abb:Referenzverlauf_Mittelwerte_der_Polling-Perioden}{meanValueCPP_Referenzverlauf_MeanValueRanges}
\importsmallgnuplotps{Standardabweichung Mittelwerte $cpp$: Referenz}
{Abb:Standardabweichung_Referenzverlauf_Mittelwerte_der_Polling-Perioden}{stdDevCPP_Referenzverlauf_MeanValueRanges}
\importsmallgnuplotps{Variationskoeffizient Mittelwerte $cpp$: Referenz}
{Abb:Variationskoeffizient_Referenzverlauf_Mittelwerte_der_Polling-Perioden}{coeffVarCPP_Referenzverlauf_MeanValueRanges}
\importsmallgnuplotps{Nachrichtenverzögerung $\Gamma_V$: Referenz}{Abb:Referenzverlauf_Nachrichtenverzoegerung}{avgMsgDelayRatio_Referenzverlauf_MeanValueRanges}
\importsmallgnuplotps{Aktualitätsgrad $\Gamma_A$: Referenz}{Abb:Referenzverlauf_Aktualitaetsgrad}{avgUptodateRatio_Referenzverlauf_MeanValueRanges}
\importsmallgnuplotps{Prozentsatz der gesparten Nachrichten}{Abb:Referenzverlauf_Prozentsatz_der_gesparten_Nachrichten}{relReOmRatio_Referenzverlauf_MeanValueRanges}

Abbildung \ref{Abb:Referenzverlauf_Mittelwerte_der_Polling-Perioden} zeigt die Entwicklung der Mittelwerte der Polling-Perioden aller Subscriber. Nach einem Anstieg
bis ca. zum Zeitpunkt 3000 Sekunden zirkulieren die mittleren Polling-Perioden um einen gedachten Mittelwert von ca. 50.000 Sekunden.\\

Abbildung \ref{Abb:Standardabweichung_Referenzverlauf_Mittelwerte_der_Polling-Perioden} zeigt die Standardabweichung von den Mittelwerten der Polling-Perioden.
Mit Zunahme der Mittelwerte nimmt also auch in gleichem Maße die Streuung zu. Bemerkenswert ist, dass trotz immer noch starker Schwankungen der Mittelwerte
und einer großen Streuung die Serverbelastung stabil bleibt (siehe Abbildung \ref{Abb:Referenzverlauf}). Um auch die Größe der Mittelwerte zu berücksichtigen,
haben wir in Abbildung \ref{Abb:Variationskoeffizient_Referenzverlauf_Mittelwerte_der_Polling-Perioden} den normierten Wert der
Standardabweichung, den Variationskoeffizienten, (Standardabweichung geteilt durch den Mittelwert) dargestellt. Der Einfluss der Größe eines Mittelwerts 
auf die Streuung wird hierbei unterbunden. Es zeigt sich, dass der Variationskoeffizient nach einer gewissen Zeit relativ stabil bleibt und
mit der Serverbelastung korreliert.\\

Abbildung \ref{Abb:Referenzverlauf_Nachrichtenverzoegerung} zeigt den Graphen unserer in Abschnitt \ref{angestrebte_dienstguete} definierten Funktion
$\Gamma_V$. Auffällig ist, dass direkt nach Abschluss der Beitrittsphase die Nachrichtenverzögerung am niedrigsten ist (bis auf die
Anfangsphase). Vergleicht man mit Abbildung \ref{Abb:Referenzverlauf}, so ist zu erkennen, dass die Anpassung der Polling-Perioden fast
zeitgleich abgeschlossen ist und der Server zu diesem Zeitpunkt optimal ausgelastet ist.\\

Die Entwicklung korreliert ebenfalls mit dem Graphen unserer in Abschnitt \ref{angestrebte_dienstguete} definierten Funktion
$\Gamma_A$ (siehe Abbildung \ref{Abb:Referenzverlauf_Aktualitaetsgrad}). Die anfängliche Spitze im Graphen der Abbildung
\ref{Abb:Referenzverlauf_Aktualitaetsgrad} ist dadurch zu erklären, dass sich zunächst wenige Subscriber im Netzwerk befinden und der
Server in diesem Zeitbereich noch wenig belastet ist. Daher können die Nachrichten schnell durch das Notifikationssystem
übermittelt werden. Ein Zusammenhang zwischen Serverbelastung, Nachrichtenverzögerung und
Aktualitätsgrad ist also deutlich zu erkennen. \\

Abbildung \ref{Abb:Referenzverlauf_Prozentsatz_der_gesparten_Nachrichten} zeigt den prozentualen Anteil der gesparten Nachrichten im Gegensatz zu einem System ohne
Verteilung der Feeds über ein Notifikationssystem. Grundsätzlich gilt: je geringer die Rate der Anfragen und je größer die Serverbelastung, desto höher ist
der prozentuale Anteil gesparter Nachrichten. Denn in diesen Fällen erhält ein Subscriber einen Feed eher über das Notifikationssystem als direkt vom
RSS-Server. Im Graph in Abbildung \ref{Abb:Referenzverlauf_Prozentsatz_der_gesparten_Nachrichten} ist zu sehen, dass nach Abschluss der Beitrittsphase
zum Zeitpunkt 1000 der Anteil an gesparten Nachrichten wieder leicht sinkt. Diese Abnahme hat ihre Ursache in der geringeren Serverbelastung als im davor
liegenden Zeitraum.

\subsection{Staukontrolle}
Der graphischen Verlauf der Serverbelastung mit Staukontrolle ist dem Referenzgraphen in Abbildung \ref{Abb:Referenzverlauf} zu entnehmen.\\

Man kann sich leicht überlegen, welche Folgen eine fehlende Staukontrolle seitens der Subscriber hat. Die Auswirkungen einer fehlenden Staukontrolle
auf die Serverbelastung haben wir ebenfalls untersucht. In Abbildung \ref{Abb:Ohne_Staukontrolle} ist zu sehen,
dass die Serverbelastung wie erwartet stetig zunimmt.

\importsmallgnuplotps{Serverbelastung: Ohne Staukontrolle}{Abb:Ohne_Staukontrolle}{ToTR_NoCongCont_MVR}
\importsmallgnuplotps{Aktualitätsgrad $\Gamma_A$ (ohne Staukontrolle)}
{Abb:Ohne_Staukontrolle_Aktualitaetsgrad}{avgUptodateRatio_NoCongCont_MeanValueRanges}
\importsmallgnuplotps{Nachrichtenverzögerung $\Gamma_V$ (ohne Staukontrolle)}
{Abb:Ohne_Staukontrolle_Nachrichtenverzoegerung}{avgMsgDelayRatio_NoCongCont_MeanValueRanges}

Der Grad der Nachrichtenverzögerung nimmt ebenfalls stetig zu (siehe Abbildung \ref{Abb:Ohne_Staukontrolle_Nachrichtenverzoegerung}),
wogegen der Aktualitätsgrad praktisch auf null sinkt (siehe Abbildung \ref{Abb:Ohne_Staukontrolle_Aktualitaetsgrad}).

\subsection{Ausbalancierung}

\importsmallgnuplotps{Serverbelastung: Keine Ausbalancierung}{Abb:Keine_Ausbalancierung}{ToTR_NoBalancing_MVR}
\importsmallgnuplotps{Serverbelastung: Ausbalancierung}{Abb:Ausbalancierung}{ToTR_Balancing_MVR}
\importsmallgnuplotps{Mittelwerte der Polling-Perioden ($cpp$): keine Ausbalancierung}
{Abb:Keine_Ausbalancierung_Mittelwerte_der_Polling-Perioden}{meanValueCPP_NoBalancing_MeanValueRanges}
\importsmallgnuplotps{Mittelwerte der Polling-Perioden ($cpp$): Ausbalancierung}
{Abb:Ausbalancierung_Mittelwerte_der_Polling-Perioden}{meanValueCPP_Balancing_MeanValueRanges}
\importsmallgnuplotps{Standardabweichung: Mittelwerte $cpp$: k. Ausbalancierung}
{Abb:Keine_Ausbalancierung_Standardabweichung_der_Mittelwerte_der_Polling-Perioden}{stdDevCPP_NoBalancing_MeanValueRanges}
\importsmallgnuplotps{Standardabweichung Mittelwerte $cpp$: Ausbalancierung}
{Abb:Ausbalancierung_Standardabweichung_der_Mittelwerte_der_Polling-Perioden}{stdDevCPP_Balancing_MeanValueRanges}
\importsmallgnuplotps{Variationskoeffizient Mittelwerte $cpp$: k. Ausbalancierung}
{Abb:Keine_Ausbalancierung_Variationskoeffizient_der_Mittelwerte_der_Polling-Perioden}{coeffVarCPP_NoBalancing_MeanValueRanges}
\importsmallgnuplotps{Variationskoeffizient Mittelwerte $cpp$: Ausbalancierung}
{Abb:Ausbalancierung_Variationskoeffizient_der_Mittelwerte_der_Polling-Perioden}{coeffVarCPP_Balancing_MeanValueRanges}

In diesem Experiment soll untersucht werden, wie sich ein Ausbleiben der Ausbalancierung der Polling-Perioden auf die Adaption der Polling-Perioden auswirkt und ob es
zum Effekt des ``Aussperrens'' kommt (siehe Abschnitt \ref{cs:ausbalancierung_der_polling-perioden}). Um den Effekt zu maximieren, haben wir bei diesem Experiment
auf ein allmähliches Beitreten der Subscriber zum Overlay-Netzwerk verzichtet. Alle Subscriber treten gleichzeitig zu Beginn der Simulation dem Overlay-Netzwerk
bei. Kommt es zum ``Aussperren'', so sollte dieser Effekt anhand der Anzahl der Anfragen an den RSS-Server deutlich sichtbar sein, denn alle ausgesperrten
Subscriber werden aufgrund ausbleibender Antworten ihre Anfragen wiederholen. Dies muss nahezu gleichzeitig geschehen, da alle Subscriber zur selben Zeit dem
Netzwerk beigetreten sind und ihre $RT$s daher annähernd zur gleichen Zeit ablaufen werden. Die Mittelwerte der Polling-Perioden aller Subscriber sollten sich
kontinuierlich erhöhen. Denn nur eine geringe Zahl der Subscriber hat Zugriff auf den Server und kann ihre Polling-Perioden konstant niedrig halten. Die übrigen
Subscriber werden ihre Polling-Perioden erhöhen.\\
Diese Effekte sind im Graphen in Abbildung \ref{Abb:Keine_Ausbalancierung} gut nachvollziehbar. Die auftretenden Spitzen zeigen, dass zu dieser Zeit sehr viele
Subscriber gleichzeitig Feed-Requests aussenden. Dies kann nur eine Ursache in gleichzeitig ablaufenden $RT$s haben. Die Zeitdifferenzen nehmen exponentiell zu.
Dies entspricht der exponentiellen Zunahme der $rto$s. Nach jeder auftretenden Spitze ist die Queue kaum gefüllt, ihr Füllgrad steigt jedoch daraufhin wieder.
Erklären lässt sich dies folgendermaßen: die nicht ausgesperrten Subscriber müssen ihren $cpp$ ebenfalls drosseln (entsprechend den ausgesperrten Subscribern) und
können diesen daraufhin jedoch allmählich wieder steigern.\\
Schaut man sich die Entwicklung der mittleren Polling-Perioden ($cpp$) in Abbildung
\ref{Abb:Keine_Ausbalancierung_Mittelwerte_der_Polling-Perioden} an, so erkennt man (neben der stetigen Steigerung der Polling-Perioden) eine unmittelbare
Korrelation mit den auftretenden Spitzen im Graphen der Abbildung \ref{Abb:Keine_Ausbalancierung}. Tatsächlich steigern also größtenteils die ausgesperrten
Subscriber ihre Polling-Perioden. Sie erhalten keine Möglichkeit, diese zu senken. Auch die Standardabweichung von den Mittelwerten der Polling-Perioden (Abbildung
\ref{Abb:Keine_Ausbalancierung_Standardabweichung_der_Mittelwerte_der_Polling-Perioden}) unterstützen die These des ``Aussperrens''. Die Graphik zeigt,
dass die Polling-Perioden der verschiedenen Subscriber stark von einander abweichen. Abbildung
\ref{Abb:Keine_Ausbalancierung_Variationskoeffizient_der_Mittelwerte_der_Polling-Perioden} zeigt den Variationskoeffizienten.\\
Als Vergleichsgraphen haben wir in Abbildung \ref{Abb:Ausbalancierung} den graphischen Verlauf der Serverbelastung mit Ausbalancierung bei ansonsten gleicher
Parameterwahl dargestellt. Bei Vergleich der Graphen der Mittelwerte beider
Verfahren fällt auf, dass sich die Mittelwerte in beiden Fällen (nach völlig unterschiedlichem Verlauf)
etwa bei 100.000 Sekunden bewegen. Jedoch haben sich die Mittelwerte bei dem ausbalancierten Verfahren nach unten hin eingepegelt (Abbildung
\ref{Abb:Ausbalancierung_Mittelwerte_der_Polling-Perioden}), während die Mittelwerte beim nicht-ausbalancierten Verfahren weiterhin stetig steigen
(Abbildung \ref{Abb:Keine_Ausbalancierung_Mittelwerte_der_Polling-Perioden}).
Ein großer Unterschied zwischen beiden Verfahren ist bei dem Vergleich der Standardabweichungen zu erkennen: beim nicht-ausbalancierten Verfahren
(Abbildung \ref{Abb:Keine_Ausbalancierung_Standardabweichung_der_Mittelwerte_der_Polling-Perioden}) hat zum Ende der Simulation die Standardabweichung
den Wert 180.000 erreicht und ist weiter steigend; beim ausbalancierten Verfahren (Abbildung
\ref{Abb:Ausbalancierung_Standardabweichung_der_Mittelwerte_der_Polling-Perioden}) hat sich die Standardabweichung nach unten hin
auf ca. den Wert 100.000 eingepegelt. Der Variationskoeffizient des ausbalancierten Verfahrens
(Abbildung \ref{Abb:Ausbalancierung_Variationskoeffizient_der_Mittelwerte_der_Polling-Perioden}) pegelt sich ebenfalls auf einem niedrigeren Wert ein,
als der Variationskoeffizient des nicht-ausbalancierten Verfahrens
(Abbildung \ref{Abb:Keine_Ausbalancierung_Variationskoeffizient_der_Mittelwerte_der_Polling-Perioden}) .

\subsection{Churn-Kompensation}
\label{exp:churn_kompensation}

\suppressfloats

\importsmallgnuplotps{Serverbelastung: Ohne Churn-Kompensation}{Abb:ohne_Churn-Kompensation}{ToTR_NoChurnCompensation50_80_900_MVR}
\importsmallgnuplotps{Serverbelastung: Mit Churn-Kompensation}{Abb:mit_Churn-Kompensation}{ToTR_ChurnCompensation50_80_900_MVR}

\importsmallgnuplotps{Serverbelastung: Exponentielle Skalierungsfunktion}
{Abb:Exponentielle_Skalierungsfunktion}{ToTR_ExponentialArttFactor_serviceTimeFactor400_MVR}
\importsmallgnuplotps{Serverbelastung: Polynomielle Skalierungsfunktion}
{Abb:Polynomielle_Skalierungsfunktion}{ToTR_QuadraticArttFactor_serviceTimeFactor400_MVR}

Mit Hilfe dieses Experiments soll der Einflusses von Churn auf die Serverbelastung sowohl mit als auch ohne Churn-Kompensation ermittelt werden.
Je stärker die schon bestehende Serverbelastung bzw. je weniger leistungsfähig ein Server ist, desto größer ist der Einfluss von Churn.
Damit der Einfluss von Churn auf die Serverbelastung deutlich hervor tritt, wurde in diesem Experiment ein $serviceTimeFactor$ von 50 gewählt.
Die Nachrichtenlaufzeit ist von entscheidender Bedeutung für die Churn-Kompensation: ist sie zu hoch, dann benötigt die Übermittlung des
$InitialBrokerRSSFeed$ sehr viel Zeit, so dass ein Subscriber zwischenzeitlich bereits den RSS-Server kontaktiert.
Die gewählten Werte für Nachrichtenlaufzeiten sind im Vergleich zu in der Realität auftretenden Werten sehr hoch gewählt.
Allerdings wollen wir das System unter möglichst ungünstigen Bedingungen testen. Bei geringeren Übertragungsgeschwindigkeiten verbessert sich
die Auswirkung der Churn-Kompensation. Die hier dargestellten Werte sind Extremwerte und sollten unter realen Bedingungen weit besser ausfallen.\\

Churn beginnt beim Zeitpunkt 3000 und endet beim Zeitpunkt 5000. Dabei sind nach 900 Sekunden 80\% der Klienten ausgetauscht.
Abbildung \ref{Abb:ohne_Churn-Kompensation} zeigt den Einfluss von Churn auf die Serverbelastung ohne Churn-Kompensation. Zu erkennen ist eine starke
und stetig ansteigende Serverbelastung. Nur ab etwa dem Zeitpunkt 4600 Sekunden fällt die Kurve wieder leicht ab. Nach Beendigung der Churnphase
pegelt sich die Kurve langsam wieder in Richtung Server-Queue ein. Dagegen zeigt der Graph mit Churn-Kompensation in
Abbildung \ref{Abb:mit_Churn-Kompensation} eine weit geringere Serverbelastung und ein besseres Adaptionsverhalten, obwohl auch hier der Einfluss
von Churn nicht ganz vermeidbar ist. Es fällt weiterhin auf, dass durch die Churn-Kompensation schon in der Beitrittsphase (0..1000 Sekunden)
ein besseres Adaptionsverhalten erreicht wird.\\

Auf die Entwicklung der Serverbelastung bei Wahl der Standard-Parameter hat Churn keinen nennenswerten Einfluss (hier nicht dargestellt).

\subsection{Bestimmung des $artt$ -- Skalierungsfunktion für den $rtt$}
\label{exp:skalierung}

Bei diesem Experiment soll die Adaptionsgüte der Polling-Perioden anhand der Serverbelastung bei unterschiedlicher Wahl der Skalierungsfunktion
sf() (siehe Abschnitt \ref{css:staukontrolle_pubsubrss}) ermittelt werden. Wir beschränken uns dabei auf den Vergleich exponentieller und
polynomieller Funktionen. Auch hier haben wir einen $serviceTimeFactor$ von 400 gewählt, um Unterschiede in der Entwicklung deutlicher sichtbar
zu machen. Abbildung \ref{Abb:Exponentielle_Skalierungsfunktion} zeigt den Verlauf der Serverbelastung bei Wahl der
Skalierungsfunktion $\text{sf}(x):=2^{x-1}$. In Abbildung \ref{Abb:Polynomielle_Skalierungsfunktion} ist der Verlauf der Serverbelastung bei
$\text{sf}(x):=x^2$ zu sehen. Unterschiede im Verlauf sind zwar sichtbar, der Vorteil der exponentiellen Skalierungsfunktion ist jedoch
relativ klein.

\subsection{Reaktion auf Netzwerkveränderung}
Dieses Experiment dient zur Untersuchung, wie das System auf plötzliche Veränderungen der Netzwerkgröße (Anzahl der Klienten im System)
reagiert. Es ist wünschenswert, dass sich das System schnell an die veränderte Situation anpasst und dass es dabei nicht zu Seiteneffekten
(wie z. B. ein starkes Schwingungsverhalten) kommt. Für diesen Zweck verlassen in der Simulation die Hälfte der Klienten zum Zeitpunkt
2000 das Overlay-Netzwerk. Zum Zeitpunkt 4000 tritt die gleiche Anzahl an Klienten dem Netzwerk wieder bei. Abbildung
\ref{Abb:Ploetzliche_Veraenderung_der_Netzwerkgroesse} zeigt den graphischen Verlauf. Es ist zu erkennen, dass das Verlassen der Klienten
nur eine kurzzeitige Senkung des Füllgrades der Server-Queue bewirkt. Der Pegel steigt anschließend wieder bis zur oberen Grenze.
Der Beitritt der Klienten zum System bewirkt zwar eine kurzzeitige starke Mehrbelastung des Servers, das System kann diese Mehrbelastung jedoch
relativ schnell ausgleichen.

\newpage
\subsection{Steigerung des $rto$}
\importsmallgnuplotps{Plötzliche Veränderung der Netzwerkgröße}{Abb:Ploetzliche_Veraenderung_der_Netzwerkgroesse}{ToTR_SubscribersLeave50_MVR}
Hier soll gezeigt werden, dass eine exponentielle Steigerung des $rto$ ebenfalls ein besseres Adaptionsverhalten als eine polynomielle Steigerung
nach sich zieht.
Für dieses Experiment wurde zunächst ein $serviceTimeFactor$ von 1 gewählt. Nach 2000 Sekunden wird dieser auf 400 hochgesetzt, so dass der Server
nur noch stark zeitverzögert antworten kann. Abbildung \ref{Abb:Exponentielle_Steigerung_des_rto} zeigt die Adaptionsgüte bei exponentieller Steigerung,
Abbildung \ref{Abb:Polynomielle_Steigerung_des_rto} die Adaptionsgüte bei polynomieller Steigerung des $rto$. Hier ist ein deutlich besserer
Verlauf der Adaption bei exponentieller Steigerung zu sehen.

\importsmallgnuplotps{Serverbelastung: Exponentielle Steigerung des $rto$}{Abb:Exponentielle_Steigerung_des_rto}{ToTR_Exponential_MVR}
\importsmallgnuplotps{Serverbelastung: Polynomielle Steigerung des $rto$}{Abb:Polynomielle_Steigerung_des_rto}{ToTR_Polynomial_MVR}

\importsmallgnuplotps{Serverbelastung: Queuegröße 3000}{Abb:Queuegroesse_3000}{ToTR_q3000_MVR}
\importsmallgnuplotps{Mittelwerte $cpp$: Queuegröße 3000}{Abb:Mittelwerte_Queuegroesse_3000}{meanValueCPP_queue3000_MeanValueRanges}
\importsmallgnuplotps{Standardabweichung Mittelwerte $cpp$: Queuegröße 3000}{Abb:Standardabweichung_Mittelwerte_Queuegroesse_3000}{stdDevCPP_queue3000_MeanValueRanges}
\importsmallgnuplotps{Variationskoeffizient Mittelwerte $cpp$: Queuegröße 3000}{Abb:Variationskoeffizient_Mittelwerte_Queuegroesse_3000}{coeffVarCPP_queue3000_MeanValueRanges}

\subsection{Queuegrößen}
Bisher haben wir eine im Verhältnis zu Anzahl der Klienten sehr kleine Queuegröße für die Server-Queue gewählt. In diesem Experiment wollen wir
untersuchen, wie sich eine große Queue auf das Adaptionsverhalten auswirkt. Dafür haben wir eine Queue gewählt, welche 3000 Feed-Requests
aufnehmen kann, also mehr, als sich Klienten im Netzwerk befinden. Ist die Queue groß genug, so sollten nur wenige oder gar keine Feed-Requests
verworfen werden, da die Klienten genug Spielraum haben, um ihre Polling-Perioden anzupassen. Es ist zu erwarten, dass die Polling-Perioden
im weiteren Verlauf nur geringen Schwankungen unterliegen, da nur verworfene Feed-Requests zu zufälligen und unverhältnismäßig großen
Steigerungen führen. Weiterhin sollten die Differenzen der Polling-Perioden zwischen den verschiedenen Klienten relativ gering sein,
da die Bedingungen für alle Subscriber gleich sind und es nicht zu zufälligen Steigerungen kommen kann. Abbildung \ref{Abb:Queuegroesse_3000}
zeigt die Entwicklung der Adaption. Es treten keine verworfenen Feed-Requests auf. Nach einer gewissen Zeit (etwa 5000 Sekunden) hat sich die
Kurve auf einem festen Wert (1500) eingepegelt. Das bedeutet, dass ab diesem Zeitpunkt ein ausgewogenes Verhältnis zwischen
mittlerer Ankunftsrate der Feed-Requests und mittlerer Bearbeitungszeit erreicht ist. Abbildung \ref{Abb:Mittelwerte_Queuegroesse_3000}
zeigt die Entwicklung der Mittelwerte der Polling-Perioden. Hier ist zu sehen, dass sich die Mittelwerte entsprechend unseren Erwartungen
auf einem Wert (etwa 2000) einpegeln und keinen weiteren Schwankungen unterliegen. Die Abbildungen
\ref{Abb:Standardabweichung_Mittelwerte_Queuegroesse_3000} und \ref{Abb:Variationskoeffizient_Mittelwerte_Queuegroesse_3000}
machen deutlich, dass die Abweichungen von den Mittelwerten relativ gering sind. Besonders im Vergleich mit den Referenzgraphen (Abbildungen
\ref{Abb:Referenzverlauf_Mittelwerte_der_Polling-Perioden}, \ref{Abb:Standardabweichung_Referenzverlauf_Mittelwerte_der_Polling-Perioden}
und \ref{Abb:Variationskoeffizient_Referenzverlauf_Mittelwerte_der_Polling-Perioden}) zeigt sich, dass bei der Wahl einer großen Queue
(bei ansonsten gleicher Parameterwahl) die Mittelwerte und die Streuungen viel geringere Werte annehmen und es somit zu einer
homogeneren Verteilung der Mittelwerte auf die verschiedenen Klienten kommt.

\subsection{Reaktion auf Belastungsänderung}
Hier soll untersucht werden, wie das System auf Änderungen des Belastungsgrades eines Servers reagiert. Dabei bestehen je nach Dauer der
Belastungsänderung unterschiedliche Anforderungen an das System.

\importsmallgnuplotps{Serverbelastung: langzeitige Überbelastung}{Abb:langzeitige_Ueberbelastung}{ToTR_FastIncreasing_MVR}
\importsmallgnuplotps{Serverbelastung: langzeitige Belastungsminderung}{Abb:langzeitige_Belastungsminderung}{ToTR_FastDegrading_MVR}
\importsmallgnuplotps{Serverbelastung: kurzzeitige Überbelastung}{Abb:kurzzeitige_Ueberbelastung}{ToTR_SuddenIncreaseDecrease40_MVR}
\importsmallgnuplotps{Serverbelastung: kurzzeitige Belastungsminderung}{Abb:kurzzeitige_Belastungsminderung}{ToTR_SuddenDecreaseIncrease40_MVR}
\paragraph{Langzeitige Belastungsänderungen:}
Bei einer langzeitigen Belastungsänderung eines Servers fordern wir vom System (bzw. von den Klienten) eine möglichst rasche Adaption an die
neuen Bedingungen. Abbildung \ref{Abb:langzeitige_Ueberbelastung} zeigt den Verlauf der Adaption bei einer 50-fachen Steigerung der
Serverbelastung ($serviceTimeFactor=50$).
Zu erkennen ist ein logarithmischer Verlauf der Kurve, welcher unseren Erwartungen entspricht, da sowohl die Steigerung des $rto$ als auch
die Skalierungsfunktion $\text{sf}()$ exponentiell gewählt sind (Abbildung \ref{Abb:Exponentielle_Steigerung_des_rto} des vorigen Abschnitts
zeigt den Verlauf bei einer 400-fach gesteigerten Serverbelastung). Den umgekehrten Fall (Wechsel von 50-facher auf Standard-Serverbelastung)
spiegelt Abbildung \ref{Abb:langzeitige_Belastungsminderung} wider. Die Anpassung geschieht deutlich schneller, da Klienten ihren $cpp$
nicht allmählich drosseln, sondern diesen auf die tatsächlich gemessene roundtrip-time setzen.

\paragraph{Kurzzeitige Belastungsänderungen:}
Ist ein Server für einen
kurzen Zeitraum nicht in der Lage, Anfragen in angemessener Zeit zu bearbeiten, so wäre es nicht wünschenswert, wenn das System (bzw. die Klienten)
mit einer starken und länger andauernden Reaktion überreagiert. Abbildung \ref{Abb:kurzzeitige_Ueberbelastung} zeigt die Reaktion des Systems
auf eine 50-fach gesteigerte Serverbelastung, welche nach 40 Sekunden auf den ursprünglichen Wert zurückgesetzt wurde.
Wie zu sehen ist, ändert sich der Verlauf der Kurve nur ganz geringfügig. Abbildung
\ref{Abb:kurzzeitige_Belastungsminderung} zeigt auch hier den umgekehrten Fall bei einem ursprünglichen $serviceTimeFactor$ von 50.
Kurzzeitige Belastungsminderungen sind allerdings in der Realität eher weniger wahrscheinlich.

\importsmallgnuplotps{Serverbelastung: allmähliche Überbelastung}{Abb:allmaehliche_Ueberbelastung}{ToTR_SoftIncreasing_MVR}
\importsmallgnuplotps{Serverbelastung: allmähliche Minderbelastung}{Abb:allmaehliche_Minderbelastung}{ToTR_SoftDegrading_MVR}

\importsmallgnuplotps{Serverbelastung: $mpp=3600$}{Abb:mpp_3600}{ToTR_mpp3600_serviceTimeFactor50_MVR}
\importsmallgnuplotps{Serverbelastung: $mpp=910800$}{Abb:mpp_910800}{ToTR_mpp910800_serviceTimeFactor50_MVR}

\paragraph{Allmähliche Belastungsänderungen:}
Bei einer allmählichen Belastungsänderung fordern wir, dass sich das System so schnell an den veränderten Belastungsgrad anpasst, dass
eine zusätzliche Mehr- bzw. Minderbelastung ausbleibt. Abbildungen \ref{Abb:allmaehliche_Ueberbelastung} und
\ref{Abb:allmaehliche_Minderbelastung} zeigen als Beispiele die Graphen der Adaptionsverläufe bei allmählicher Mehr- bzw. Minderbelastung
bei einer graduellen Steigerung des $serviceTimeFactors$ um den Wert 1 alle 40 Sekunden. Je kleiner die graduelle Steigerung
bzw. je größer das Zeitintervall zwischen den graduellen Änderungen, desto homogener und unauffälliger ist die Adaption. 

\subsection{Größe des $mpp$}
Damit die Polling-Perioden nicht in unermessliche Höhen schießen (beispielsweise aufgrund von Fehlfunktionen), haben wir den $mpp$
definiert. Dieser sollte groß genug gewählt werden, so dass eine Adaption unter jeden Umständen vollständig ist. Wie sich ein zu
gering gewählter $mpp$ auf das Adaptionsverhalten auswirkt, sehen wir exemplarisch in Abbildung \ref{Abb:mpp_3600} bei einem
$mpp$ von 3600 und einem $serviceTimeFactor$ von 50. Den Verlauf der Adaption bei $mpp=910800$ unter ansonsten gleichen Bedingungen ist in Abbildung
\ref{Abb:mpp_910800} zu sehen. Bei $mpp=3600$ wird eine vollständige Adaption verhindert, da der $cpp$ eines Klienten nicht groß genug
gewählt werden kann.
%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "diplomarbeit"
%%% End: 
